{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPHsiDxj2ohT"
      },
      "source": [
        "# Resume Extraction & Embedding Pipeline\n",
        "\n",
        "## How to Use:\n",
        "\n",
        "### Option 1: Quick Run (Recommended)\n",
        "1. Run **Cell 1** - Install packages\n",
        "2. Run **Cell 2** - Setup and configure APIs\n",
        "3. Run **Cell 11 (Last Cell)** - This will execute the entire pipeline\n",
        "\n",
        "### Option 2: Step by Step\n",
        "Run cells in order (1-11) to see detailed output at each step.\n",
        "\n",
        "## What the Pipeline Does:\n",
        "1. ✓ Uploads your resume PDF\n",
        "2. ✓ Parses PDF and extracts text\n",
        "3. ✓ Generates metadata (file hash, timestamps)\n",
        "4. ✓ Sends to Gemini API for structured parsing\n",
        "5. ✓ Builds embedding text (skills, experience, projects)\n",
        "6. ✓ Generates vector embeddings using BAAI/bge-base-en-v1.5\n",
        "7. ✓ Exports results to JSON file\n",
        "\n",
        "## Output:\n",
        "- Extracted resume data (JSON)\n",
        "- Vector embeddings\n",
        "- File saved to: `./resumes/{file_hash}.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nWK5BfdmZg3",
        "outputId": "395d3683-570a-4a96-97be-edbb0dfdd66f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/328.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.2/328.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n",
            "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 5.1.2\n",
            "    Uninstalling sentence-transformers-5.1.2:\n",
            "      Successfully uninstalled sentence-transformers-5.1.2\n",
            "Successfully installed sentence-transformers-5.2.0\n",
            "Packages installed successfully\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf google-generativeai python-dotenv requests -q\n",
        "!pip install -U sentence-transformers\n",
        "print(\"Packages installed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any,Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "YfxPkVexooJB",
        "outputId": "aaf39414-d732-43df-c1b8-ff91f2ad69af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Gemini API configured\n",
            "✓ Hugging Face token found\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from hashlib import sha256\n",
        "import io\n",
        "from pypdf import PdfReader\n",
        "import requests\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "from google.colab import userdata\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configure APIs\n",
        "GEMINI_API_KEY1 = userdata.get('GEMINI_API_KEY_COLLEGE_ID')\n",
        "HF_TOKEN =  userdata.get('HF_TOKEN')\n",
        "\n",
        "\n",
        "if GEMINI_API_KEY1:\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    print(\"✓ Gemini API configured\")\n",
        "else:\n",
        "    print(\"⚠ GEMINI_API_KEY not found in environment\")\n",
        "\n",
        "if HF_TOKEN:\n",
        "    print(\"✓ Hugging Face token found\")\n",
        "else:\n",
        "    print(\"⚠ HF_TOKEN not found in environment\")\n",
        "\n",
        "# Global variables for storing results\n",
        "resume_text = None\n",
        "file_name = None\n",
        "metadata = None\n",
        "parsed_resume = None\n",
        "embedding_text = None\n",
        "embedding = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QuJSDixTpRqN"
      },
      "outputs": [],
      "source": [
        "def upload_resume():\n",
        "    \"\"\"Upload multiple resume files from user.\"\"\"\n",
        "    from google.colab import files\n",
        "    import time\n",
        "\n",
        "    print(\"Please upload one or more resume PDF files...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Give the system time to write all files\n",
        "    time.sleep(2)\n",
        "\n",
        "    resume_file_paths = []\n",
        "\n",
        "    # Iterate through all uploaded files\n",
        "    for file_name in uploaded.keys():\n",
        "        # Handle Colab's file renaming (adds \"(1)\", \"(2)\", etc.)\n",
        "        actual_file_name = file_name\n",
        "\n",
        "        # Try the original name first\n",
        "        if os.path.exists(actual_file_name):\n",
        "            file_size = os.path.getsize(actual_file_name)\n",
        "            resume_file_paths.append((actual_file_name, file_name))\n",
        "            print(f\"✓ Resume file found: {file_name}\")\n",
        "            print(f\"  - File size: {file_size} bytes\")\n",
        "        else:\n",
        "            print(f\"⚠ File not found: {actual_file_name}\")\n",
        "\n",
        "    if not resume_file_paths:\n",
        "        raise Exception(\"No files uploaded successfully\")\n",
        "\n",
        "    print(f\"\\n✓ Total files uploaded: {len(resume_file_paths)}\")\n",
        "    return resume_file_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DLz1YQuNpwEK"
      },
      "outputs": [],
      "source": [
        "def parse_pdf(resume_file_path):\n",
        "    \"\"\"Parse PDF and extract text.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 1: PARSE PDF\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    try:\n",
        "        with open(resume_file_path, 'rb') as pdf_file:\n",
        "            reader = PdfReader(pdf_file)\n",
        "            print(f\"Total pages: {len(reader.pages)}\")\n",
        "\n",
        "            resume_text = \"\"\n",
        "            for page_num, page in enumerate(reader.pages):\n",
        "                page_text = page.extract_text()\n",
        "                resume_text += page_text\n",
        "                print(f\"  - Page {page_num + 1}: {len(page_text)} characters\")\n",
        "\n",
        "        print(f\"\\n✓ Successfully extracted {len(resume_text)} characters\")\n",
        "        print(f\"\\nPreview (first 300 characters):\")\n",
        "        print(\"-\" * 40)\n",
        "        print(resume_text[:300])\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        return resume_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error parsing PDF: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qBNWn-jGp-cB"
      },
      "outputs": [],
      "source": [
        "def extract_metadata(resume_text, file_name):\n",
        "    \"\"\"Extract and generate metadata from resume.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 2: EXTRACT METADATA\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    def generate_hash_value(text: str) -> str:\n",
        "        \"\"\"Generate SHA256 hash of normalized resume text.\"\"\"\n",
        "        normalized = text.strip().lower()\n",
        "        normalized = normalized.replace('\\r\\n', '\\n')\n",
        "        normalized = re.sub(r'\\b(page|pages)\\s+\\d+\\b', '', normalized)\n",
        "        normalized = re.sub(r'[^\\w\\s]', ' ', normalized)\n",
        "        normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
        "        return sha256(normalized.encode()).hexdigest()\n",
        "\n",
        "    file_hash = generate_hash_value(resume_text)\n",
        "\n",
        "    metadata = {\n",
        "        \"fileName\": file_name,\n",
        "        \"fileHash\": file_hash,\n",
        "        \"parsedAt\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"parserVersion\": \"1.0.0\",\n",
        "        \"Language\": \"en\"\n",
        "    }\n",
        "\n",
        "    print(f\"Metadata extracted:\")\n",
        "    print(json.dumps(metadata, indent=2))\n",
        "\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8Gh0fyLjqCNX"
      },
      "outputs": [],
      "source": [
        "def extract_resume_with_llm(resume_text):\n",
        "    \"\"\"Send resume text to LLM for parsing.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 3: LLM EXTRACTION - PARSING RESUME\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    resume_parsing_prompt = \"\"\"Return only valid JSON that exactly matches the ParsedResume schema described below. Do NOT add or remove fields, explanations, comments, code fences, markdown, or any text outside the JSON. The output must be a pure JSON object that can be parsed by JSON.parse() with no trailing characters. If a value cannot be determined with reasonable confidence, use null. Dates must follow ISO 8601 (full datetime with Z when possible, or \"YYYY-MM-DD\", \"YYYY-MM\", or \"YYYY\"). For fuzzy dates, use the FlexibleDate structure. Trim all strings and deduplicate lists. Do not fabricate information.\n",
        "\n",
        "The response MUST be a raw JSON object.\n",
        "Do NOT use markdown.\n",
        "Do NOT use triple backticks.\n",
        "Do NOT wrap the output in ```json or ```.\n",
        "If any non-JSON character is produced, the output is invalid.\n",
        "\n",
        "TOP-LEVEL RULES\n",
        "\n",
        "Always return a JSON object containing exactly the keys defined in the schema.\n",
        "\n",
        "id: required string; you may generate a deterministic ID such as \"resume_main\" if nothing else is known.\n",
        "\n",
        "If a sub-object is optional and no data exists, use null or empty arrays as required by the schema.\n",
        "\n",
        "Avoid guessing; when uncertain, use null.\n",
        "\n",
        "\n",
        "\n",
        "No metadata is required or allowed in output.\n",
        "\n",
        "SCHEMA (NO METADATA FIELD)\n",
        "\n",
        "The top-level object must contain exactly these keys:\n",
        "\n",
        "{\n",
        "  \"id\": string,\n",
        "  \"analysis\": ResumeAnalysis,\n",
        "  \"verification\": VerificationFlags,\n",
        "  \"basics\": Basics,\n",
        "  \"skills\": SkillProfile[],\n",
        "  \"workExperience\": WorkExperience[],\n",
        "  \"education\": Education[],\n",
        "  \"projects\": Project[],\n",
        "  \"certifications\": Certification[],\n",
        "  \"languages\": Language[]\n",
        "}\n",
        "\n",
        "1. analysis (ResumeAnalysis)\n",
        "{\n",
        "  \"quality\": {\n",
        "    \"score\": number,                   // 0–100\n",
        "    \"level\": \"low\"|\"average\"|\"high\"|\"exceptional\",\n",
        "    \"hints\": [\"string\"]\n",
        "  },\n",
        "  \"suspicion\": {\n",
        "    \"score\": number,                   // 0–100\n",
        "    \"level\": \"safe\"|\"concern\"|\"suspicious\"|\"high_risk\",\n",
        "    \"flags\": [\n",
        "      {\n",
        "        \"type\": \"string\",\n",
        "        \"severity\": \"low\"|\"medium\"|\"critical\",\n",
        "        \"description\": \"string\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  \"writingStyle\": {\n",
        "    \"actionVerbsRate\": number,         // 0.0–1.0\n",
        "    \"quantificationRate\": number,      // 0.0–1.0\n",
        "    \"clicheCount\": number\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "Quality level mapping:\n",
        "0–40 low, >40–70 average, >70–90 high, >90 exceptional.\n",
        "\n",
        "2. verification (VerificationFlags)\n",
        "{\n",
        "  \"timeline\": {\n",
        "    \"hasGaps\": boolean,\n",
        "    \"gaps\": [\n",
        "      {\n",
        "        \"startDate\": IsoDate,\n",
        "        \"endDate\": IsoDate,\n",
        "        \"durationDays\": number\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  \"identity\": {\n",
        "    \"geoConsistency\": \"match\"|\"mismatch\"|\"unknown\",\n",
        "    \"socialFootprintFound\": boolean\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "Detect gaps between jobs/education > 60 days.\n",
        "\n",
        "Social footprint refers to existence of LinkedIn/GitHub/etc. signals in the résumé.\n",
        "\n",
        "3. basics (Basics)\n",
        "{\n",
        "  \"name\": Traceable<string>,\n",
        "  \"email\": Traceable<string>[],\n",
        "  \"phone\": Traceable<string>[],\n",
        "  \"location\": Location,\n",
        "  \"urls\": [\n",
        "    {\n",
        "      \"type\": \"linkedin\"|\"github\"|\"portfolio\"|\"personal\",\n",
        "      \"url\": \"string\"\n",
        "    }\n",
        "  ],\n",
        "  \"summary\": \"string|null\"\n",
        "}\n",
        "\n",
        "Traceable<T>\n",
        "{\n",
        "  \"value\": T,\n",
        "  \"rawText\": string,\n",
        "  \"confidence\": number,     // 0–1\n",
        "  \"pageIndex\": number|null\n",
        "}\n",
        "\n",
        "Location\n",
        "{\n",
        "  \"rawInput\": string,\n",
        "  \"city\": string|null,\n",
        "  \"state\": string|null,\n",
        "  \"country\": string|null,\n",
        "  \"zipCode\": string|null,\n",
        "  \"countryCode\": string|null\n",
        "}\n",
        "\n",
        "\n",
        "Rules:\n",
        "\n",
        "Emails must be lowercase, valid format only.\n",
        "\n",
        "Phones must be normalized to E.164 when possible; digits-only otherwise.\n",
        "\n",
        "Summary ≤ 800 chars.\n",
        "\n",
        "4. skills: SkillProfile[]\n",
        "{\n",
        "  \"name\": string,\n",
        "  \"normalizedName\": string,\n",
        "  \"category\": string,\n",
        "  \"computedLevel\": \"novice\"|\"intermediate\"|\"advanced\"|\"expert\",\n",
        "  \"validityScore\": number, // 0–10\n",
        "  \"metadata\": {\n",
        "    \"firstSeen\": IsoDate,\n",
        "    \"lastUsed\": IsoDate,\n",
        "    \"totalMonthsExperience\": number,\n",
        "    \"occurrenceCount\": number,\n",
        "    \"sources\": [\n",
        "      {\n",
        "        \"sectionId\": string,\n",
        "        \"sectionType\": \"experience\"|\"education\"|\"project\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "Canonicalize skill names.\n",
        "\n",
        "Deduplicate strongly.\n",
        "\n",
        "Max 200 skills.\n",
        "\n",
        "5. workExperience: WorkExperience[]\n",
        "{\n",
        "  \"id\": string,\n",
        "  \"title\": Traceable<string>,\n",
        "  \"normalizedTitle\": string|null,\n",
        "  \"company\": Traceable<string>,\n",
        "  \"companyDomain\": string|null,\n",
        "  \"location\": Location|null,\n",
        "  \"type\": \"full-time\"|\"contract\"|\"internship\"|null,\n",
        "  \"startDate\": FlexibleDate,\n",
        "  \"endDate\": FlexibleDate,\n",
        "  \"description\": string|null,\n",
        "  \"responsibilities\": [\"string\"],\n",
        "  \"skillsDetected\": [\"string\"],\n",
        "  \"isVerified\": boolean,\n",
        "  \"verificationNotes\": string|null,\n",
        "  \"verificationConfidence\": number|null,\n",
        "  \"verificationDate\": IsoDate|null\n",
        "}\n",
        "\n",
        "FlexibleDate\n",
        "{\n",
        "  \"rawText\": string,\n",
        "  \"isoDate\": IsoDate|null,\n",
        "  \"isCurrent\": boolean\n",
        "}\n",
        "\n",
        "\n",
        "Rules:\n",
        "\n",
        "Most recent first.\n",
        "\n",
        "Ongoing roles → endDate.isoDate = null, isCurrent = true.\n",
        "\n",
        "Responsibilities ≤ 200 chars each.\n",
        "\n",
        "If title.value is present, normalize it using a canonical job title taxonomy (e.g., \"Sr.\" → \"Senior\", \"SDE\" → \"Software Engineer\", remove company-specific prefixes). If normalization cannot be performed with high confidence, set normalizedTitle to null. Never invent seniority or role scope.\n",
        "\n",
        "\n",
        "skillsDetected: canonicalized skill names.\n",
        "\n",
        "6. education: Education[]\n",
        "{\n",
        "  \"id\": string,\n",
        "  \"institution\": Traceable<string>,\n",
        "  \"degree\": Traceable<string>,\n",
        "  \"normalizedDegree\": \"high_school\"|\"bachelors\"|\"masters\"|\"phd\"|null,\n",
        "  \"fieldOfStudy\": string|null,\n",
        "  \"startDate\": FlexibleDate|null,\n",
        "  \"endDate\": FlexibleDate|null,\n",
        "  \"gpa\": {\n",
        "    \"score\": number,\n",
        "    \"scale\": number\n",
        "  } | null\n",
        "}\n",
        "\n",
        "\n",
        "Normalize degree types:\n",
        "\n",
        "BSc/BS/Bachelor → \"bachelors\"\n",
        "\n",
        "MSc/MS/Master → \"masters\"\n",
        "\n",
        "PhD/Doctorate → \"phd\"\n",
        "\n",
        "7. projects: Project[]\n",
        "{\n",
        "  \"name\": string,\n",
        "  \"description\": string|null,\n",
        "  \"url\": string|null,\n",
        "  \"skillsUsed\": [\"string\"]\n",
        "}\n",
        "\n",
        "8. certifications: Certification[]\n",
        "{\n",
        "  \"name\": string,\n",
        "  \"issuer\": string,\n",
        "  \"date\": FlexibleDate,\n",
        "  \"doesExpire\": boolean,\n",
        "  \"verificationUrl\": string|null\n",
        "}\n",
        "\n",
        "9. languages: Language[]\n",
        "{\n",
        "  \"language\": string,\n",
        "  \"proficiency\": \"native\"|\"fluent\"|\"conversational\"|\"basic\"\n",
        "}\n",
        "\n",
        "PARSING RULES\n",
        "\n",
        "Trim all strings.\n",
        "\n",
        "Deduplicate arrays case-insensitively.\n",
        "\n",
        "Preserve ordering by relevance.\n",
        "\n",
        "Maximum items:\n",
        "\n",
        "experience: 50\n",
        "\n",
        "education: 50\n",
        "\n",
        "skills: 200\n",
        "\n",
        "projects: 50\n",
        "\n",
        "Never invent details; use null when not confident.\n",
        "\n",
        "Remove unsafe content (scripts, hidden text, encoded payloads).\n",
        "\n",
        "FINAL INSTRUCTION\n",
        "\n",
        "Use this entire prompt as-is. Append the resume text inside the wrapper.\n",
        "Return only the JSON object that conforms exactly to this schema.\n",
        "No markdown. No explanations. No extra text.\n",
        "ENTERPRISE SCORING & DETERMINISTIC RULES (APPEND-ONLY)\n",
        "\n",
        "The following rules define EXACT, deterministic methods to compute all scores and flags.\n",
        "These rules are authoritative. Do not invent alternative heuristics.\n",
        "\n",
        "================================================================\n",
        "A. SKILL VALIDITY SCORE (skills[].validityScore)\n",
        "================================================================\n",
        "\n",
        "Range: 0.0 – 10.0 (float allowed, round to 1 decimal)\n",
        "\n",
        "Purpose:\n",
        "Measures confidence that a skill is real, relevant, and supported by evidence in the résumé.\n",
        "\n",
        "Formula:\n",
        "\n",
        "validityScore =\n",
        "  (\n",
        "    0.30 * occurrenceFactor +\n",
        "    0.25 * recencyFactor +\n",
        "    0.20 * corroborationFactor +\n",
        "    0.15 * experienceFactor +\n",
        "    0.10 * sourceReliabilityFactor\n",
        "  ) * 10\n",
        "\n",
        "All factors are clamped to 0.0 – 1.0.\n",
        "\n",
        "Definitions:\n",
        "\n",
        "1. occurrenceFactor\n",
        "   = min(1.0, log(1 + occurrenceCount) / log(1 + 20))\n",
        "\n",
        "2. recencyFactor\n",
        "   - If lastUsed is null → 0.0\n",
        "   - Else:\n",
        "       monthsSinceLastUse = months between lastUsed and now\n",
        "       recencyFactor =\n",
        "         monthsSinceLastUse <= 6  → 1.0\n",
        "         <= 12                    → 0.8\n",
        "         <= 24                    → 0.6\n",
        "         <= 48                    → 0.4\n",
        "         > 48                     → 0.2\n",
        "\n",
        "3. corroborationFactor\n",
        "   = min(1.0, number of distinct sectionTypes in sources / 3)\n",
        "   (experience, project, education)\n",
        "\n",
        "4. experienceFactor\n",
        "   - If totalMonthsExperience is null → 0.0\n",
        "   - Else:\n",
        "       experienceFactor = min(1.0, totalMonthsExperience / 60)\n",
        "\n",
        "5. sourceReliabilityFactor\n",
        "   - Appears in workExperience → 1.0\n",
        "   - Appears only in projects → 0.7\n",
        "   - Appears only in education → 0.6\n",
        "   - Appears only in summary/skills list → 0.4\n",
        "\n",
        "ComputedLevel Mapping (skills[].computedLevel):\n",
        "- validityScore < 3.0        → novice\n",
        "- 3.0 – 5.9                  → intermediate\n",
        "- 6.0 – 8.4                  → advanced\n",
        "- ≥ 8.5                      → expert\n",
        "\n",
        "================================================================\n",
        "B. RESUME QUALITY SCORE (analysis.quality)\n",
        "================================================================\n",
        "\n",
        "Range: 0 – 100 (integer)\n",
        "\n",
        "quality.score =\n",
        "  0.30 * structureScore +\n",
        "  0.30 * contentDepthScore +\n",
        "  0.20 * clarityScore +\n",
        "  0.20 * consistencyScore\n",
        "\n",
        "Each component normalized to 0–100.\n",
        "\n",
        "1. structureScore\n",
        "   - Presence of basics, experience, skills, education\n",
        "   - +25 per major section present (max 100)\n",
        "\n",
        "2. contentDepthScore\n",
        "   - Average responsibilities per role ≥ 3 → 100\n",
        "   - ≥ 2 → 75\n",
        "   - ≥ 1 → 50\n",
        "   - else → 25\n",
        "\n",
        "3. clarityScore\n",
        "   - Based on writingStyle:\n",
        "     clarityScore =\n",
        "       (actionVerbsRate * 50) +\n",
        "       (quantificationRate * 40) -\n",
        "       (min(clicheCount, 10) * 2)\n",
        "\n",
        "   Clamp 0–100.\n",
        "\n",
        "4. consistencyScore\n",
        "   - No overlapping dates → 100\n",
        "   - Minor overlaps or fuzzy dates → 70\n",
        "   - Multiple conflicts → 40\n",
        "\n",
        "quality.level mapping:\n",
        "0–40 low\n",
        ">40–70 average\n",
        ">70–90 high\n",
        ">90 exceptional\n",
        "\n",
        "================================================================\n",
        "C. SUSPICION SCORE (analysis.suspicion)\n",
        "================================================================\n",
        "\n",
        "Range: 0 – 100\n",
        "\n",
        "Start at 0, add penalties:\n",
        "\n",
        "+20  unexplained timeline gap > 12 months\n",
        "+15  multiple overlapping full-time roles\n",
        "+15  excessive buzzwords without evidence\n",
        "+10  skills listed but never used\n",
        "+10  inconsistent locations across roles\n",
        "+30  fabricated-looking company names or dates\n",
        "\n",
        "Clamp to 100.\n",
        "\n",
        "suspicion.level:\n",
        "0–20     → safe\n",
        "21–40    → concern\n",
        "41–70    → suspicious\n",
        ">70      → high_risk\n",
        "\n",
        "================================================================\n",
        "D. WRITING STYLE METRICS (analysis.writingStyle)\n",
        "================================================================\n",
        "\n",
        "actionVerbsRate\n",
        "= (# bullet points starting with action verb) / (total bullet points)\n",
        "\n",
        "quantificationRate\n",
        "= (# bullet points containing numbers, %, $, metrics) / (total bullet points)\n",
        "\n",
        "clicheCount\n",
        "= count of overused phrases (e.g., \"hard-working\", \"team player\", \"go-getter\")\n",
        "\n",
        "================================================================\n",
        "E. WORK EXPERIENCE VERIFICATION RULES\n",
        "================================================================\n",
        "\n",
        "isVerified = true ONLY IF:\n",
        "- Company domain exists AND\n",
        "- Role dates are consistent AND\n",
        "- Skill usage aligns with role title\n",
        "\n",
        "Else isVerified = false.\n",
        "\n",
        "verificationConfidence (0.0 – 1.0):\n",
        "- 1.0 → all signals match\n",
        "- 0.7 → partial corroboration\n",
        "- 0.4 → weak evidence\n",
        "- null → no verification attempted\n",
        "\n",
        "================================================================\n",
        "F. TIMELINE GAP DETECTION\n",
        "================================================================\n",
        "\n",
        "A gap exists if:\n",
        "(endDate of previous role) → (startDate of next role) > 60 days\n",
        "\n",
        "durationDays must be exact.\n",
        "\n",
        "================================================================\n",
        "G. TRACEABLE CONFIDENCE RULES\n",
        "================================================================\n",
        "\n",
        "Traceable.confidence:\n",
        "- Exact match (email, phone, URL) → 1.0\n",
        "- Minor normalization → 0.9\n",
        "- Heuristic extraction → 0.7\n",
        "- Inferred / ambiguous → 0.4\n",
        "\n",
        "================================================================\n",
        "H. GLOBAL SAFETY & DETERMINISM\n",
        "================================================================\n",
        "\n",
        "- Never infer skills, companies, or dates not explicitly present.\n",
        "- Scores must be explainable using the rules above.\n",
        "- If required inputs are missing, degrade score deterministically.\n",
        "- No randomness. Same input must produce same output.\n",
        "\n",
        "END OF ENTERPRISE EXTENSIONS\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    resume_parsing_prompt = resume_parsing_prompt + \"\\n===START===\\n\" + resume_text + \"\\n===END===\"\n",
        "\n",
        "    print(\"Sending resume text to Gemini API for parsing...\")\n",
        "    print(f\"Prompt size: {len(resume_parsing_prompt)} characters\\n\")\n",
        "\n",
        "    try:\n",
        "        print(\"hello world\")\n",
        "        model = genai.GenerativeModel(\"gemini-flash-latest\")\n",
        "        response = model.generate_content(resume_parsing_prompt)\n",
        "\n",
        "        print(f\"✓ Received response from Gemini API\")\n",
        "        print(f\"Response size: {len(response.text)} characters\\n\")\n",
        "\n",
        "        parsed_resume = json.loads(response.text)\n",
        "\n",
        "        print(\"✓ Successfully parsed JSON response\")\n",
        "        print(f\"\\nResume ID: {parsed_resume.get('id')}\")\n",
        "        print(f\"Quality Score: {parsed_resume.get('analysis', {}).get('quality', {}).get('score')}\")\n",
        "        print(f\"Quality Level: {parsed_resume.get('analysis', {}).get('quality', {}).get('level')}\")\n",
        "\n",
        "        return parsed_resume\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"✗ Failed to parse LLM response as JSON: {e}\")\n",
        "        print(f\"Response preview: {response.text[:500]}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error during LLM extraction: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uYAep0FTrnia"
      },
      "outputs": [],
      "source": [
        "def add_metadata_to_resume(parsed_resume, metadata):\n",
        "    \"\"\"Add metadata to parsed resume.\"\"\"\n",
        "    parsed_resume['metaData'] = metadata\n",
        "\n",
        "    print(\"✓ Metadata added to parsed resume\")\n",
        "    print(f\"\\nMetadata in resume:\")\n",
        "    print(json.dumps(parsed_resume['metaData'], indent=2))\n",
        "\n",
        "    return parsed_resume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8XO5qjx1r7Lw"
      },
      "outputs": [],
      "source": [
        "def build_embedding_text(parsed_resume):\n",
        "    \"\"\"Build embedding text from parsed resume.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 4: BUILD EMBEDDING TEXT\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    skills = parsed_resume.get('skills', [])\n",
        "    top_skills = sorted(\n",
        "        skills,\n",
        "        key=lambda s: s.get('validityScore', 0),\n",
        "        reverse=True\n",
        "    )[:20]\n",
        "\n",
        "    skills_text = \", \".join([s['name'] for s in top_skills])\n",
        "    print(f\"Top 20 skills extracted: {len(top_skills)} skills\")\n",
        "    print(f\"Skills: {skills_text}\\n\")\n",
        "\n",
        "    experiences = parsed_resume.get('workExperience', [])[:3]\n",
        "    experiences_text = \"\"\n",
        "    for i, exp in enumerate(experiences, 1):\n",
        "        title = exp.get('normalizedTitle') or exp.get('title', {}).get('value', 'Unknown')\n",
        "        description = exp.get('description') or '; '.join(exp.get('responsibilities', [])[:2])\n",
        "        line = f\"{title},{description}\"\n",
        "        experiences_text += line + \"\\n\"\n",
        "        print(f\"{i}. {title}\")\n",
        "        print(f\"   {description[:80]}...\\n\")\n",
        "\n",
        "    experiences_text = experiences_text.strip()\n",
        "\n",
        "    projects = parsed_resume.get('projects', [])[:3]\n",
        "    projects_text = \"\"\n",
        "    for i, proj in enumerate(projects, 1):\n",
        "        line = f\"{proj.get('name')}: {proj.get('description', 'No description')}\"\n",
        "        projects_text += line + \"\\n\"\n",
        "        print(f\"Project {i}: {proj.get('name')}\")\n",
        "        print(f\"  {(proj.get('description') or 'No description')[:80]}...\\n\")\n",
        "\n",
        "    projects_text = projects_text.strip()\n",
        "\n",
        "    education = parsed_resume.get('education', [])\n",
        "    education_text = \"\"\n",
        "    for i, edu in enumerate(education, 1):\n",
        "        degree = edu.get('normalizedDegree', 'Degree').replace('_', ' ').title()\n",
        "        field = edu.get('fieldOfStudy', 'Unspecified')\n",
        "        line = f\"{degree} in {field}\"\n",
        "        education_text += line + \"\\n\"\n",
        "        print(f\"Education {i}: {line}\\n\")\n",
        "\n",
        "    education_text = education_text.strip()\n",
        "\n",
        "    embedding_text = f\"\"\"Experience:\n",
        "{experiences_text}\n",
        "\n",
        "Projects:\n",
        "{projects_text}\n",
        "\n",
        "Skills:\n",
        "{skills_text}\n",
        "\n",
        "Education:\n",
        "{education_text}\"\"\".strip()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EMBEDDING TEXT BUILT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nTotal characters: {len(embedding_text)}\")\n",
        "    print(f\"\\nPreview:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(embedding_text)\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    return embedding_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NzBiAgRHsy-k"
      },
      "outputs": [],
      "source": [
        "def generate_embeddings(embedding_text):\n",
        "    \"\"\"Generate vector embeddings.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 5: GENERATE VECTOR EMBEDDINGS\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(f\"Generating embeddings for {len(embedding_text)} characters...\\n\")\n",
        "\n",
        "    try:\n",
        "        model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "        embedding = model.encode(\n",
        "            embedding_text,\n",
        "            normalize_embeddings=True\n",
        "        )\n",
        "\n",
        "        print(\"✓ Embeddings generated successfully\")\n",
        "        print(f\"  - Embedding dimensions: {len(embedding)}\")\n",
        "        print(f\"  - Number of vectors: 1\")\n",
        "        print(\"\\nFirst 10 embedding values:\")\n",
        "        print(embedding[:10])\n",
        "\n",
        "        return embedding\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error generating embeddings: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZlhkHGt4ySt_"
      },
      "outputs": [],
      "source": [
        "def display_and_export(parsed_resume, metadata, embedding_text, embedding):\n",
        "    \"\"\"Display results and export to JSON file.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXTRACTED RESUME DATA (JSON)\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(json.dumps(parsed_resume, indent=2))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXPORTING RESULTS\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    EXPORT_DIR = \"./resumes\"\n",
        "    os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "    file_hash = metadata[\"fileHash\"]\n",
        "    output_file = os.path.join(EXPORT_DIR, f\"{file_hash}.json\")\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"✗ Duplicate detected: resume with hash {file_hash} already exists\")\n",
        "        print(f\"  - Path: {output_file}\")\n",
        "    else:\n",
        "        output_data = {\n",
        "            \"metadata\": metadata,\n",
        "            \"extractedResume\": parsed_resume,\n",
        "            \"embeddingText\": embedding_text,\n",
        "            \"embeddingInfo\": {\n",
        "                \"model\": \"BAAI/bge-base-en-v1.5\",\n",
        "                \"dimensions\": len(embedding),\n",
        "                \"vectorCount\": 1,\n",
        "                \"embedding\": embedding.tolist()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(output_data, f, indent=2)\n",
        "\n",
        "        print(f\"✓ Resume data exported successfully\")\n",
        "        print(f\"  - Path: {output_file}\")\n",
        "        print(f\"  - File size: {os.path.getsize(output_file)} bytes\")\n",
        "        print(\"  - Contains: metadata, extracted resume, embedding text, embedding info\")\n",
        "\n",
        "    return output_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQS5rQzYyg5t",
        "outputId": "501f94b1-e6c1-4ae5-d961-e554b2b45480"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 0: UPLOAD RESUMES\n",
            "============================================================\n",
            "\n",
            "Please upload one or more resume PDF files...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8040f563-8999-44ab-a931-12e37c5e95ae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8040f563-8999-44ab-a931-12e37c5e95ae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "async def process_resume_pipeline():\n",
        "    \"\"\"Main pipeline - run this cell to process multiple resumes.\"\"\"\n",
        "    try:\n",
        "        # Step 0: Upload resumes\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 0: UPLOAD RESUMES\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "        resume_file_paths = upload_resume()\n",
        "\n",
        "        total_files = len(resume_file_paths)\n",
        "        successful_files = 0\n",
        "        failed_files = 0\n",
        "        processed_files = []\n",
        "\n",
        "        # Process each resume\n",
        "        for file_index, (resume_file_path, file_name) in enumerate(resume_file_paths, 1):\n",
        "            print(\"\\n\" + \"█\"*60)\n",
        "            print(f\"PROCESSING FILE {file_index}/{total_files}: {file_name}\")\n",
        "            print(\"█\"*60 + \"\\n\")\n",
        "\n",
        "            try:\n",
        "                # Step 1: Parse PDF\n",
        "                resume_text = parse_pdf(resume_file_path)\n",
        "\n",
        "                # Step 2: Extract metadata\n",
        "                metadata = extract_metadata(resume_text, file_name)\n",
        "\n",
        "                # Step 3: LLM extraction\n",
        "                parsed_resume = extract_resume_with_llm(resume_text)\n",
        "\n",
        "                # Step 4: Add metadata\n",
        "                parsed_resume = add_metadata_to_resume(parsed_resume, metadata)\n",
        "\n",
        "                # Step 5: Build embedding text\n",
        "                embedding_text = build_embedding_text(parsed_resume)\n",
        "\n",
        "                # Step 6: Generate embeddings\n",
        "                embedding = generate_embeddings(embedding_text)\n",
        "\n",
        "                # Step 7: Display and export\n",
        "                output_file = display_and_export(parsed_resume, metadata, embedding_text, embedding)\n",
        "\n",
        "                processed_files.append({\n",
        "                    \"fileName\": file_name,\n",
        "                    \"fileHash\": metadata[\"fileHash\"],\n",
        "                    \"outputFile\": output_file,\n",
        "                    \"qualityScore\": parsed_resume.get('analysis', {}).get('quality', {}).get('score'),\n",
        "                    \"status\": \"✓ SUCCESS\"\n",
        "                })\n",
        "                successful_files += 1\n",
        "\n",
        "                print(f\"\\n✓ File {file_index}/{total_files} processed successfully\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n✗ Error processing {file_name}: {e}\")\n",
        "                processed_files.append({\n",
        "                    \"fileName\": file_name,\n",
        "                    \"status\": f\"✗ FAILED: {str(e)[:100]}\"\n",
        "                })\n",
        "                failed_files += 1\n",
        "\n",
        "        # Final summary\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"BATCH PROCESSING COMPLETE\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\nTotal files: {total_files}\")\n",
        "        print(f\"✓ Successful: {successful_files}\")\n",
        "        print(f\"✗ Failed: {failed_files}\")\n",
        "\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "        print(\"PROCESSING SUMMARY:\")\n",
        "        print(\"-\"*60)\n",
        "        for i, result in enumerate(processed_files, 1):\n",
        "            print(f\"\\n{i}. {result['fileName']}\")\n",
        "            print(f\"   Status: {result['status']}\")\n",
        "            if result['status'].startswith('✓'):\n",
        "                print(f\"   Hash: {result['fileHash']}\")\n",
        "                print(f\"   Quality Score: {result['qualityScore']}/100\")\n",
        "                print(f\"   Output: {result['outputFile']}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"✓ PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Pipeline failed: {e}\")\n",
        "        raise\n",
        "\n",
        "# RUN THIS CELL TO START THE ENTIRE PIPELINE\n",
        "import asyncio\n",
        "await process_resume_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MFo_Li8rypA"
      },
      "source": [
        "# Resume Filtering Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKBMutqQOiXU"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'Optional' from 'datetime' (c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\datetime.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m date, datetime, Optional\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'Optional' from 'datetime' (c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\datetime.py)"
          ]
        }
      ],
      "source": [
        "from datetime import date, datetime\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLnYx_x6r3fw",
        "outputId": "a2407303-6d16-4ebf-eff0-190575d0d31f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['326e891505900873d99554bcfc54a599dd81b8a65d0cfbe24398cc1bfe52d499.json', '5de19d5713bffdb8eea75ba42eb9f7f1fb77109bbacbfd952da1a2a1495ed6a9.json', '8647c7180442e944180d4681273c247b1f2876474de361e2cdfb88f1111f64fc.json', 'a172fdd98d8dc172f165e67742a88b6da6737da06a6f9286dba1380305d49bb8.json', 'a1a490916a30e5e00123c8229538cef99a8047b6682dc4ce0d0b25f3c4449b37.json']\n"
          ]
        }
      ],
      "source": [
        "# Directory containing parsed resume JSON files\n",
        "RESUME_DIR = \"./resumes/parsed\"\n",
        "\n",
        "def load_all_resumes(resume_dir: str) -> List[Dict[str,Any]]:\n",
        "  resumes = []\n",
        "  for file in os.listdir(RESUME_DIR):\n",
        "    if file.endswith(\".json\"):\n",
        "      with open(os.path.join(resume_dir,file),\"r\",encoding=\"utf-8\") as f:\n",
        "        resumes.append(json.load(f))\n",
        "  return resumes\n",
        "print(os.listdir(RESUME_DIR))\n",
        "allResume = load_all_resumes(RESUME_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWgfgoHsLDId"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to check quality gate\n",
        "def passes_quality_gate(resume:Dict[str,Any],min_quality:int=60,max_suspicion:int=40):\n",
        "  quality_score = resume.get(\"extractedResume\",{}).get(\"analysis\",{}).get(\"quality\",{}).get(\"score\",0)\n",
        "  suspicion_score = resume.get(\"extractedResume\",{}).get(\"analysis\",{}).get(\"suspicion\",{}).get(\"score\",0)\n",
        "  return quality_score >= min_quality and suspicion_score <= max_suspicion\n",
        "  \n",
        "passes_quality_gate(allResume[0],min_quality=70,max_suspicion=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rn6R-g1MCeE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# I built this because the default iso date parser in python is too strict for our use case since dates can be in YYYY, YYYY-MM or YYYY-MM-DD format\n",
        "def getDate(date):\n",
        "  if len(date) == 10:\n",
        "    return datetime.fromisoformat(date).date()\n",
        "  elif len(date) == 7:\n",
        "    return datetime.strptime(date,\"%Y-%m\").date()\n",
        "  elif len(date) == 4:\n",
        "    return datetime.strptime(date,\"%Y\").date()\n",
        "  return 0\n",
        "  \n",
        "# Calculate total months between two dates given in FlexibleDate format\n",
        "def getTotalMonths(startDate:Dict[str,Any],endDate:Dict[str,Any]):\n",
        "  if not startDate or not endDate or not startDate.get(\"isoDate\"):\n",
        "    return 0\n",
        "  try:\n",
        "    startDate = getDate(startDate.get(\"isoDate\"))\n",
        "  except:\n",
        "    return 0\n",
        "  if endDate and endDate.get(\"isCurrent\") is True:\n",
        "    endDate = date.today()\n",
        "  elif endDate and endDate.get(\"isoDate\"):\n",
        "    try:\n",
        "      endDate = getDate(endDate.get(\"isoDate\"))\n",
        "    except:\n",
        "      endDate = date.today()\n",
        "  else:\n",
        "    endDate = date.today()\n",
        "  year_diff = endDate.year - startDate.year\n",
        "  month_diff = endDate.month - startDate.month\n",
        "  total_months = year_diff * 12 + month_diff\n",
        "  return max(total_months,0)\n",
        "\n",
        "def industry_experience_gate(resume:Dict[str,Any],min_industry_experience:int=36):\n",
        "  total_months = 0\n",
        "  workExperiences = resume.get(\"extractedResume\",{}).get(\"workExperience\",{})\n",
        "  for workExperience in workExperiences:\n",
        "    total_months += getTotalMonths(workExperience.get(\"startDate\"),workExperience.get(\"endDate\"))\n",
        "  print(total_months)\n",
        "  return total_months >= min_industry_experience\n",
        "  \n",
        "industry_experience_gate(allResume[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgxwSSAeK9jh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEGREE_RANK = {\n",
        "    \"high_school\": 1,\n",
        "    \"diploma\": 2,\n",
        "    \"associate\": 2,\n",
        "    \"bachelors\": 3,\n",
        "    \"masters\": 4,\n",
        "    \"phd\": 5,\n",
        "    \"doctorate\": 5\n",
        "}\n",
        "def education_gate(candidate_education: List[Dict[str,Any]],required_degrees:List[str],required_fields: Optional[List[str]]=None,isCurrent:bool = False):\n",
        "    for edu in candidate_education:\n",
        "        degree = edu.get(\"normalizedDegree\")\n",
        "        fieldOfStudy = edu.get(\"fieldOfStudy\",\"\").lower()\n",
        "        endDate = edu.get(\"endDate\",{})\n",
        "        hasRequiredDegree = False\n",
        "        hasRequiredField = False\n",
        "        for required_degree in required_degrees:\n",
        "            if degree and DEGREE_RANK.get(required_degree,0) <= DEGREE_RANK.get(degree,0):\n",
        "                hasRequiredDegree = True\n",
        "                break\n",
        "        if not hasRequiredDegree:\n",
        "            continue\n",
        "        for required_field in (required_fields or []):\n",
        "            print(required_field.lower() in fieldOfStudy)\n",
        "            if required_field and required_field.lower() in fieldOfStudy:\n",
        "                hasRequiredField = True\n",
        "                break\n",
        "        if required_fields and not hasRequiredField:\n",
        "            break\n",
        "        if isCurrent and endDate.get(\"isCurrent\") is not True:\n",
        "            continue\n",
        "        return True\n",
        "    return False\n",
        "education_gate(\n",
        "    candidate_education=allResume[0].get(\"extractedResume\",{}).get(\"education\",[]),\n",
        "    required_degrees=[\"bachelors\",\"masters\"],\n",
        "    required_fields=[\"software\"],\n",
        "    isCurrent=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
